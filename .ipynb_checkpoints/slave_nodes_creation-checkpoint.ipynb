{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Cluster Slave Nodes Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Required Libraries ::\n",
    "* __boto3__: Required to connect as operate AWS task\n",
    "* __botocore__: Required to handle the exceptions related to boto3 tasks\n",
    "* __paramiko__: Reuired to run commands inside EC2 instances\n",
    "* __json__: To convert python native dictionaries to string, to write in files\n",
    "* __pickle__: To store configuration dictionary which will be used in next notebooks\n",
    "* __datetime__, __pprint__, __os__, __sys__, __time__: General purpose use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, botocore, paramiko\n",
    "from datetime import datetime\n",
    "import pprint, os, sys, time, json, pickle\n",
    "from botocore.exceptions import ClientError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading current status and availble details::\n",
    "* User is allowed to provide specific configurations using provided format of configuration file. If user does not provide any confguration or provides wrong configuration format, then default values will be used. Please check **README.MD** file for default values.\n",
    "\n",
    "* Along with the user defined variables, we will extract details of Stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"cluster_config.json\", \"r\") as config_file:\n",
    "        user_config = json.load(config_file)\n",
    "\n",
    "    region = user_config.get('Region', \"us-east-1\")\n",
    "    wrk_spc_dir = user_config['WorkspaceDirectory']\n",
    "    user = user_config.get('UserName', \"root\")\n",
    "    cluster_instance_type = user_config.get('InstanceType', \"t2.micro\")\n",
    "    cluster_key_pair_path = user_config['KeyPairPath']\n",
    "    cluster_key_pair_name = user_config['KeyPairName']\n",
    "    project_tag = user_config.get('ProjectTag', \"SparkCluster\")\n",
    "    slave_count = user_config.get('SlaveCount', 3)\n",
    "    pickle_file = wrk_spc_dir + \"/SparkClusterOnAWSEC2_\" + user + \"_CurrentStatus.pkl\"\n",
    "    if os.path.exists(pickle_file):\n",
    "        with open(pickle_file, 'rb') as pickle_handle:\n",
    "            user_config = pickle.load(pickle_handle)\n",
    "        cluster_subnet_list = user_config['SubnetList']\n",
    "        cluster_security_group_list = [user_config['SecurityGroupId']]\n",
    "        run_id = user_config['RunId']\n",
    "    else:\n",
    "        print(\"Status file '\"+ pickle_file + \"' is not available, which is unexpected. Please start running from 'cloudformation_stack_creation.ipynb' file.\")\n",
    "        raise Exception\n",
    "except Exception as e:\n",
    "    print(\"Unexpected error while fetching available status: \" + str(e))\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating boto3 session, clients and resources ::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required Boto3 objects are defined.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    session = boto3.session.Session(region_name=region)\n",
    "    ec2_client = session.client('ec2')\n",
    "    ec2_resource = session.resource('ec2')\n",
    "    print(\"Required Boto3 objects are defined.\")\n",
    "except ClientError as e:\n",
    "    print(\"Unexpected error while creating boto3 session, client and resources: \" + str(e))\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for already running Master Node for current user ::\n",
    "* Running master node is required for any cluster to run. Checking whether master node is available for this user on not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance('i-0cda438648bad66f2') is available master node for User('ccbp-dev-user-saumalya'). Public DNS: 'ec2-3-87-179-171.compute-1.amazonaws.com'.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    master_instances = ec2_resource.instances.filter(\n",
    "        Filters=[\n",
    "            {\n",
    "                'Name': 'instance-state-name',\n",
    "                'Values': ['running']\n",
    "            },\n",
    "            {\n",
    "                'Name': 'tag:Project',\n",
    "                'Values': [project_tag]\n",
    "            },\n",
    "            {\n",
    "                'Name': 'tag:User',\n",
    "                'Values': [user]\n",
    "            },\n",
    "            {\n",
    "                'Name': 'tag:NodeType',\n",
    "                'Values': ['Master']\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "except ClientError as e:\n",
    "    print(\"Unexpected error while looking for already running Master node EC2 instance for user-'\" + user + \"': \" + str(e))\n",
    "    exit()\n",
    "try:\n",
    "    master_instance_list = list(master_instances)\n",
    "    if master_instance_list:\n",
    "        master_instance = master_instance_list[0]\n",
    "        spark_cluster_master_private_ip = master_instance.private_ip_address\n",
    "        print(\"Instance('\" + master_instance.id + \"') is available master node for User('\" + user + \"'). Public DNS: '\" + master_instance.public_dns_name + \"'.\")\n",
    "        instance_creation_flag = 1\n",
    "    else:\n",
    "        print(\"Master node is not available for User('\" + user + \"'), please create the master node first by executing 'master_node_creation.ipynb' Notebook first.\")\n",
    "        exit()\n",
    "except Exception as e:\n",
    "    print(\"Unexpected error while extracting Spark Cluster Master node details: \" + str(e))\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching latest Image id ::\n",
    "This image ID will be used to create the Slave nodes. Following configurations are already done in the Image:\n",
    "* Spark distribution is already present in the Image\n",
    "* All required packages to run pyspark is already installed in the Image\n",
    "* Jupyter notebook is configured\n",
    "* following command must be executed before spark session/context can be created using this master node:\n",
    "\n",
    "    _import findspark_\n",
    "    \n",
    "    _findspark.init(‘/home/ec2-user/spark-2.4.5-bin-hadoop2.7’)_\n",
    "\n",
    "    _import pyspark_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image('ami-0b109626cd1d1e84c') will be used to create the nodes.\n"
     ]
    }
   ],
   "source": [
    "if instance_creation_flag:\n",
    "    try:\n",
    "        node_images_list = ec2_client.describe_images(\n",
    "            Filters=[\n",
    "                {\n",
    "                    'Name': 'tag:Project',\n",
    "                    'Values': [project_tag]\n",
    "                },\n",
    "                {\n",
    "                    'Name': 'state',\n",
    "                    'Values': ['available']\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "    except ClientError as e:\n",
    "            print(\"Unexpected error while fetching node images: \" + str(e))\n",
    "            exit()\n",
    "\n",
    "    try:\n",
    "        node_image_createdates = [(datetime.strptime(img['CreationDate'][:-5], '%Y-%m-%dT%H:%M:%S'), img['ImageId']) for img in node_images_list['Images']]\n",
    "        latest_image_id = sorted(node_image_createdates, key=lambda x: x[1], reverse=True)[0][1]\n",
    "        print(\"Image('\" + latest_image_id + \"') will be used to create the nodes.\")\n",
    "    except Exception as e:\n",
    "        print(\"Unexpected error while extracting latest node image: \" + str(e))\n",
    "        exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for already running Slave Nodes for current user ::\n",
    "* If some slave nodes are already running, then same nodes will be used as slave nodes of current user.\n",
    "* To create new set of slave nodes, please terminate current slaves using 'slave_nodes_termination.ipynb', then re-run current notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if instance_creation_flag:\n",
    "    try:\n",
    "        instances = ec2_resource.instances.filter(\n",
    "            Filters=[\n",
    "                {\n",
    "                    'Name': 'instance-state-name',\n",
    "                    'Values': ['running']\n",
    "                },\n",
    "                {\n",
    "                    'Name': 'tag:Project',\n",
    "                    'Values': [project_tag]\n",
    "                },\n",
    "                {\n",
    "                    'Name': 'tag:User',\n",
    "                    'Values': [user]\n",
    "                },\n",
    "                {\n",
    "                    'Name': 'tag:NodeType',\n",
    "                    'Values': ['Slave']\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        print(\"Unexpected error while looking for already running Master node EC2 instance for user-'\" + user + \"': \" + str(e))\n",
    "        exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instanciating EC2 servers for slave nodes on AWS ::\n",
    "* __create_instance__ API is used under EC2 resource to instanciate EC2 nodes, which will be used as slave nodes of our spark cluster.\n",
    "* __Instance type__, __key-pair__ name, __subnet id__, __security group list__ is provided as decalred in previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No slave nodes are running for user-'ccbp-dev-user-saumalya'. New nodes will be created.\n",
      "Requested number of slave nodes are triggered. Will wait till those are in running state and then procceed with configuration.\n"
     ]
    }
   ],
   "source": [
    "if instance_creation_flag:\n",
    "    if list(instances):\n",
    "        for instance in instances:\n",
    "            print(\"Some slave nodes are already running for user-'\" + user + \"'. Those will be reused. If you want new set of slave nodes, please terminate those and re-run this notebook.\")\n",
    "    else:\n",
    "        print(\"No slave nodes are running for user-'\" + user + \"'. New nodes will be created.\")\n",
    "        try:\n",
    "            triggered_slave_instance_list_1 = ec2_resource.create_instances(\n",
    "                BlockDeviceMappings=[\n",
    "                    {\n",
    "                        'DeviceName': '/dev/xvda',\n",
    "                        'Ebs': {\n",
    "                            'DeleteOnTermination': True\n",
    "                        }\n",
    "                    },\n",
    "                ],\n",
    "                ImageId=latest_image_id,\n",
    "                InstanceType=cluster_instance_type,\n",
    "                KeyName=cluster_key_pair_name,\n",
    "                MaxCount=int(slave_count/2),\n",
    "                MinCount=1,\n",
    "                NetworkInterfaces=[\n",
    "                    {\n",
    "                        'DeviceIndex': 0,\n",
    "                        'SubnetId' : cluster_subnet_list[0],\n",
    "                        'Groups': cluster_security_group_list,\n",
    "                        'AssociatePublicIpAddress': True            \n",
    "                    }\n",
    "                ],\n",
    "                TagSpecifications=[\n",
    "                    {\n",
    "                        'ResourceType': 'instance',\n",
    "                        'Tags': [\n",
    "                            {\n",
    "                                'Key': 'Project',\n",
    "                                'Value': project_tag\n",
    "                            },\n",
    "                            {\n",
    "                                'Key': 'RunId',\n",
    "                                'Value': run_id\n",
    "                            },\n",
    "                            {\n",
    "                                'Key': 'User',\n",
    "                                'Value': user\n",
    "                            },\n",
    "                            {\n",
    "                                'Key': 'Name',\n",
    "                                'Value': 'SparkClusterSlave_' + str(run_id)\n",
    "                            },\n",
    "                            {\n",
    "                                'Key': 'NodeType',\n",
    "                                'Value': 'Slave'\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "            triggered_slave_instance_list_2 = ec2_resource.create_instances(\n",
    "                BlockDeviceMappings=[\n",
    "                    {\n",
    "                        'DeviceName': '/dev/xvda',\n",
    "                        'Ebs': {\n",
    "                            'DeleteOnTermination': True\n",
    "                        }\n",
    "                    },\n",
    "                ],\n",
    "                ImageId=latest_image_id,\n",
    "                InstanceType=cluster_instance_type,\n",
    "                KeyName=cluster_key_pair_name,\n",
    "                MaxCount=slave_count - int(slave_count/2),\n",
    "                MinCount=1,\n",
    "                NetworkInterfaces=[\n",
    "                    {\n",
    "                        'DeviceIndex': 0,\n",
    "                        'SubnetId' : cluster_subnet_list[1],\n",
    "                        'Groups': cluster_security_group_list,\n",
    "                        'AssociatePublicIpAddress': True            \n",
    "                    }\n",
    "                ],\n",
    "                TagSpecifications=[\n",
    "                    {\n",
    "                        'ResourceType': 'instance',\n",
    "                        'Tags': [\n",
    "                            {\n",
    "                                'Key': 'Project',\n",
    "                                'Value': project_tag\n",
    "                            },\n",
    "                            {\n",
    "                                'Key': 'RunId',\n",
    "                                'Value': run_id\n",
    "                            },\n",
    "                            {\n",
    "                                'Key': 'User',\n",
    "                                'Value': user\n",
    "                            },\n",
    "                            {\n",
    "                                'Key': 'Name',\n",
    "                                'Value': 'SparkClusterSlave_' + str(run_id)\n",
    "                            },\n",
    "                            {\n",
    "                                'Key': 'NodeType',\n",
    "                                'Value': 'Slave'\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "            triggered_slave_instance_list = triggered_slave_instance_list_1 + triggered_slave_instance_list_2\n",
    "            print('Requested number of slave nodes are triggered. Will wait till those are in ''running'' state and then procceed with configuration.')\n",
    "        except ClientError as e:\n",
    "            print(\"Unexpected error while creating Spark Cluster Master node EC2 instance for user-'\" + user + \"': \" + str(e))\n",
    "            exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching required information of the Slave Nodes ::\n",
    "* Need to iterate and probe a few times to check whether all slave nodes are up before we can fetch information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Requested EC2 nodes are not running yet. Following are the running instances. Going to sleep for 10 seconds before next probing.\n",
      "[]\n",
      "All Requested EC2 nodes are not running yet. Following are the running instances. Going to sleep for 10 seconds before next probing.\n",
      "[]\n",
      "All Requested EC2 nodes are not running yet. Following are the running instances. Going to sleep for 10 seconds before next probing.\n",
      "[{'InstanceId': 'i-0f7ebf9ec6108e5c4', 'PublicDnsName': 'ec2-3-232-95-241.compute-1.amazonaws.com'}, {'InstanceId': 'i-0a29289898713fb3d', 'PublicDnsName': 'ec2-18-209-240-76.compute-1.amazonaws.com'}]\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "All Requested EC2 slave nodes are running. Starting to configure those.\n",
      "[{'InstanceId': 'i-0f7ebf9ec6108e5c4', 'PublicDnsName': 'ec2-3-232-95-241.compute-1.amazonaws.com'}, {'InstanceId': 'i-0a29289898713fb3d', 'PublicDnsName': 'ec2-18-209-240-76.compute-1.amazonaws.com'}, {'InstanceId': 'i-073c8767a836eb1b0', 'PublicDnsName': 'ec2-54-92-187-118.compute-1.amazonaws.com'}]\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "if instance_creation_flag:\n",
    "    if triggered_slave_instance_list:\n",
    "        triggered_slave_instance_ids = [instance.id for instance in triggered_slave_instance_list]\n",
    "    else:\n",
    "        print(\"Slave instances are not triggered proeprly.\")\n",
    "        exit()\n",
    "\n",
    "    spark_cluster_slave_list = []\n",
    "    try:\n",
    "        probe_limit = 60\n",
    "        for _ in range(1, probe_limit):\n",
    "            ec2_spark_cluster_slaves = ec2_client.describe_instances(InstanceIds=triggered_slave_instance_ids)['Reservations'][0]['Instances']\n",
    "            for instance in ec2_spark_cluster_slaves:\n",
    "                if instance['State']['Code'] == 16:\n",
    "                    spark_cluster_slave_list.append({'InstanceId': instance['InstanceId'], 'PublicDnsName': instance['PublicDnsName']})\n",
    "                    triggered_slave_instance_ids.remove(instance['InstanceId'])\n",
    "            if len(spark_cluster_slave_list) < slave_count:\n",
    "                print(\"All Requested EC2 nodes are not running yet. Following are the running instances. Going to sleep for 10 seconds before next probing.\")\n",
    "                print(spark_cluster_slave_list)\n",
    "                time.sleep(10)\n",
    "            else:\n",
    "                print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "                print(\"All Requested EC2 slave nodes are running. Starting to configure those.\")\n",
    "                print(spark_cluster_slave_list)\n",
    "                print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "                break\n",
    "        else:\n",
    "            print(\"All Requested EC2 nodes are not up after 10 mins, which is not expected. Please check the status in AWS console. Remember to terminate the already running instances before running creation notebook again. Quiting process!\")\n",
    "            print(spark_cluster_slave_list)\n",
    "            exit()\n",
    "    except Exception as e:\n",
    "        print(\"Unexpected error while extracting Spark Cluster Master node details: \" + str(e))\n",
    "        exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
