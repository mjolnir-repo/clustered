{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Cluster Master node creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Required Libraries ::\n",
    "* __boto3__: Required to connect as operate AWS task\n",
    "* __botocore__: Required to handle the exceptions related to boto3 tasks\n",
    "* __paramiko__: Reuired to run commands inside EC2 instances\n",
    "* __json__: To convert python native dictionaries to string, to write in files\n",
    "* __pickle__: To store configuration dictionary which will be used in next notebooks\n",
    "* __datetime__, __pprint__, __os__, __sys__, __time__: General purpose use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, botocore, paramiko\n",
    "from datetime import datetime\n",
    "import pprint, os, sys, time, json, pickle\n",
    "from botocore.exceptions import ClientError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading current status and availble details::\n",
    "* User is allowed to provide specific configurations using provided format of configuration file. If user does not provide any confguration or provides wrong configuration format, then default values will be used. Please check **README.MD** file for default values.\n",
    "\n",
    "* Along with the user defined variables, we will extract details of Stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(\"cluster_config.json\", \"r\") as config_file:\n",
    "        user_config = json.load(config_file)\n",
    "\n",
    "    region = user_config.get('Region', \"us-east-1\")\n",
    "    wrk_spc_dir = user_config['WorkspaceDirectory']\n",
    "    user = user_config.get('UserName', \"root\")\n",
    "    cluster_instance_type = user_config.get('InstanceType', \"t2.micro\")\n",
    "    cluster_key_pair_path = user_config['KeyPairPath']\n",
    "    cluster_key_pair_name = user_config['KeyPairName']\n",
    "    project_tag = user_config.get('ProjectTag', \"SparkCluster\")\n",
    "    pickle_file = wrk_spc_dir + \"/SparkClusterOnAWSEC2_\" + user + \"_CurrentStatus.pkl\"\n",
    "    if os.path.exists(pickle_file):\n",
    "        with open(pickle_file, 'rb') as pickle_handle:\n",
    "            user_config = pickle.load(pickle_handle)\n",
    "        cluster_subnet_id = user_config['SubnetList'][0]\n",
    "        cluster_security_group_list = [user_config['SecurityGroupId']]\n",
    "        run_id = user_config['RunId']\n",
    "    else:\n",
    "        print(\"Status file '\"+ pickle_file + \"' is not available, which is unexpected. Please start running from 'cloudformation_stack_creation.ipynb' file.\")\n",
    "        raise Exception\n",
    "except Exception as e:\n",
    "    print(\"Unexpected error while fetching available status: \" + str(e))\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating boto3 session, clients and resources ::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    session = boto3.session.Session(region_name=region)\n",
    "    ec2_client = session.client('ec2')\n",
    "    ec2_resource = session.resource('ec2')\n",
    "except ClientError as e:\n",
    "    print(\"Unexpected error while creating boto3 session, client and resources: \" + str(e))\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching latest Image id ::\n",
    "This image ID will be used to create the Master node. Following configurations are already done in the Image:\n",
    "* Spark distribution is already present in the Image\n",
    "* All required packages to run pyspark is already installed in the Image\n",
    "* Jupyter notebook is configured\n",
    "* following command must be executed before spark session/context can be created using this master node:\n",
    "\n",
    "    _import findspark_\n",
    "    \n",
    "    _findspark.init(‘/home/ec2-user/spark-2.4.5-bin-hadoop2.7’)_\n",
    "\n",
    "    _import pyspark_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    node_images_list = ec2_client.describe_images(\n",
    "        Filters=[\n",
    "            {\n",
    "                'Name': 'tag:Project',\n",
    "                'Values': [project_tag]\n",
    "            },\n",
    "            {\n",
    "                'Name': 'state',\n",
    "                'Values': ['available']\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "except ClientError as e:\n",
    "    print(\"Unexpected error while fetching node images: \" + str(e))\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    node_image_createdates = [(datetime.strptime(img['CreationDate'][:-5], '%Y-%m-%dT%H:%M:%S'), img['ImageId']) for img in node_images_list['Images']]\n",
    "    latest_image_id = sorted(node_image_createdates, key=lambda x: x[1], reverse=True)[0][1]\n",
    "    latest_image_id\n",
    "except Exception as e:\n",
    "    print(\"Unexpected error while extracting latest node image: \" + str(e))\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for already running Master Node for current user ::\n",
    "* Only one master node is allowed per user.\n",
    "* If a master node is already running, then same the node will be used as master node of current user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    instances = ec2_resource.instances.filter(\n",
    "        Filters=[\n",
    "            {\n",
    "                'Name': 'instance-state-name',\n",
    "                'Values': ['running']\n",
    "            },\n",
    "            {\n",
    "                'Name': 'tag:Project',\n",
    "                'Values': [project_tag]\n",
    "            },\n",
    "            {\n",
    "                'Name': 'tag:User',\n",
    "                'Values': [user]\n",
    "            },\n",
    "            {\n",
    "                'Name': 'tag:NodeType',\n",
    "                'Values': ['Master']\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "except ClientError as e:\n",
    "    print(\"Unexpected error while looking for already running Master node EC2 instance for user-'\" + user + \"': \" + str(e))\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instanciating the EC2 for master node on AWS ::\n",
    "* __create_instance__ API is used under EC2 resource to instanciate one EC2 node, which will be Master Node of our spark cluster.\n",
    "* __Instance type__, __key-pair__ name, __subnet id__, __security group list__ is provided as decalred in previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No master node is running for user-'ccbp-dev-user-saumalya'. New node will be created.\n"
     ]
    }
   ],
   "source": [
    "if list(instances):\n",
    "    master_node_id = list(instances)[0].id\n",
    "    print(\"One master node(InstanceId-'\" + str(master_node_id) + \"') is already running for user-'\" + user + \"'. It will be reused as only one master node is allowed per user.\")\n",
    "else:\n",
    "    print(\"No master node is running for user-'\" + user + \"'. New node will be created.\")\n",
    "    try:\n",
    "        resp = ec2_resource.create_instances(\n",
    "            BlockDeviceMappings=[\n",
    "                {\n",
    "                    'DeviceName': '/dev/xvda',\n",
    "                    'Ebs': {\n",
    "                        'DeleteOnTermination': True\n",
    "                    }\n",
    "                },\n",
    "            ],\n",
    "            ImageId=latest_image_id,\n",
    "            InstanceType=cluster_instance_type,\n",
    "            KeyName=cluster_key_pair_name,\n",
    "            MaxCount=1,\n",
    "            MinCount=1,\n",
    "            NetworkInterfaces=[\n",
    "                {\n",
    "                    'DeviceIndex': 0,\n",
    "                    'SubnetId' : cluster_subnet_id,\n",
    "                    'Groups': cluster_security_group_list,\n",
    "                    'AssociatePublicIpAddress': True            \n",
    "                }\n",
    "            ],\n",
    "            TagSpecifications=[\n",
    "                {\n",
    "                    'ResourceType': 'instance',\n",
    "                    'Tags': [\n",
    "                        {\n",
    "                            'Key': 'Project',\n",
    "                            'Value': project_tag\n",
    "                        },\n",
    "                        {\n",
    "                            'Key': 'RunId',\n",
    "                            'Value': run_id\n",
    "                        },\n",
    "                        {\n",
    "                            'Key': 'User',\n",
    "                            'Value': user\n",
    "                        },\n",
    "                        {\n",
    "                            'Key': 'Name',\n",
    "                            'Value': 'SparkClusterMaster_' + str(run_id)\n",
    "                        },\n",
    "                        {\n",
    "                            'Key': 'NodeType',\n",
    "                            'Value': 'Master'\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        master_node_id = resp[0].id\n",
    "    except ClientError as e:\n",
    "        print(\"Unexpected error while creating Spark Cluster Master node EC2 instance for user-'\" + user + \"': \" + str(e))\n",
    "        exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching required information of the Master Node ::\n",
    "* Need to iterate and probe a few times to check whether the node is up before we can extract the informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requested EC2 node is still in pending mode. Going to sleep for 10 seconds before next probing.\n",
      "Requested EC2 node is still in pending mode. Going to sleep for 10 seconds before next probing.\n",
      "Requested EC2 node is still in pending mode. Going to sleep for 10 seconds before next probing.\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Requested master node is up and running. Public DNS: 'ec2-54-225-22-151.compute-1.amazonaws.com'.\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    probe_limit = 60\n",
    "    for _ in range(1, probe_limit):\n",
    "        ec2_spark_cluster_master = ec2_client.describe_instances(InstanceIds=[master_node_id])\n",
    "        if ec2_spark_cluster_master['Reservations'][0]['Instances'][0]['State']['Code'] == 16:\n",
    "            spark_cluster_master_public_dns = ec2_spark_cluster_master['Reservations'][0]['Instances'][0]['PublicDnsName']\n",
    "            spark_cluster_master_private_ip = ec2_spark_cluster_master['Reservations'][0]['Instances'][0]['PrivateIpAddress']\n",
    "            break\n",
    "        print(\"Requested EC2 node is still in \" + ec2_spark_cluster_master['Reservations'][0]['Instances'][0]['State']['Name'] + \" mode. Going to sleep for 10 seconds before next probing.\")\n",
    "        time.sleep(10)\n",
    "    else:\n",
    "        print(\"Requested EC2 node is not up after 10 mins, which is not expected. Please check the status in AWS console. Quiting process!\")\n",
    "        exit()\n",
    "    print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "    print(\"Requested master node is up and running. Public DNS: '\" + spark_cluster_master_public_dns + \"'.\")\n",
    "    print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "except Exception as e:\n",
    "    print(\"Unexpected error while extracting Spark Cluster Master node details: \" + str(e))\n",
    "    exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
