{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkClusterOnAWSEC2\n",
    "### An automated Spark Cluster on AWS EC2 instances.\n",
    "\n",
    "## 1. Introduction:\n",
    "First thing first, this utility is not to be used as a permanent business solution to create and manage a Spark cluster in any production (nor DEV in industry in that matter). Then the obvious question is why am I wasting your time?\n",
    "To answer that, let me ask a simpler question, Do you ever feel like, __\"*it would be great if you had an Spark cluster that can be turned on and off based on requirement to run your late night spark POC codes*\"__? If Yes, then you are in the right place, or else you can happily skip the entire article and I promise you will not miss out on much in your life.\n",
    "\n",
    "Now having the biggest confusion (__Is this another tutorial? - NOOOOOOO__), let's see how this can help your craving of having a small(big is also possible) and simple(complicated is also an option if you are ready to do a bit more) cluster.\n",
    "\n",
    "In a nut-shell __SparkClusterOnAWSEC2__ is exactly what the name suggests, you can create a cluster within minutes on AWS, run your code, do your work (the time consumed here is totally depends on how optimized your code is, can't help in that buddy!), and stop the cluster and clean up all services that has been on AWS __with a few clicks__. I can not stress enough on the bold part of the sentence. How to do(use) it? Let's dive in...\n",
    "\n",
    "\n",
    "#### This project focuses on launching a Spark Cluster consisting one Master node and multiple(as declared in config file) Slaves nodes. The utility has following features:\n",
    "1. Launch one EC2 instance from AWS Management Console. Download spark, configure necessary properties in the EC2 instance. Create one AMI from the EC2 and store it. (One time Activity)\n",
    "2. Create configuration file.\n",
    "3. Create one network to host the cluster using VPC, Subnets, Route Table etc.\n",
    "4. Launch Master and Slave nodes using AWS EC2. These instances are launched using one AMI, which is cretaed in Step one.\n",
    "5. Configure the Nodes to create the cluster.\n",
    "6. Start the Spark service.\n",
    "7. Stop the Spark service.\n",
    "8. Terminate the Nodes\n",
    "9. Delete network related services.\n",
    "\n",
    "We will go through each step one by one and give you enough assistance to use the utility in section 3.\n",
    "\n",
    "#### N.B.: Please note, in this version following features are not provided, those will come in future versions:\n",
    "* Add worker node to running cluster.\n",
    "* Whitelist IP in a running cluster.\n",
    "* Currently the AMI is predefined manually(although one time activity), In future releases, this step will be automated.\n",
    "* One python package version of the utility will be provided to integrate it with other applications.\n",
    "\n",
    "#### To support all these features total nine Notebooks are available. Each Notebook serves a specific purpose. These Notebooks are to executed in proper sequence to create the Cluster. The Notebooks are:\n",
    "1. cloudformation_stack_creation.ipynb\n",
    "2. master_node_creation.ipynb\n",
    "3. slave_nodes_creation.ipynb\n",
    "4. spark_cluster_configure.ipynb\n",
    "5. spark_cluster_start.ipynb\n",
    "6. spark_cluster_stop.ipynb\n",
    "7. slave_node_termination.ipynb\n",
    "8. master_node_termination.ipynb\n",
    "9. cloudformation_stack_deletion.ipynb\n",
    "\n",
    "\n",
    "## 2. Pre-Configuration:\n",
    "To use SparkClusterOnAWSEC2 utility following points must be checked. These are one time tasks, once you have done it, next time onwards you need not do these.\n",
    "1. Create AWS account, not required if already have one (https://aws.amazon.com/premiumsupport/knowledge-center/create-and-activate-aws-account).\n",
    "2. Create one IAM user having full access to EC2, Cloudformation, VPC, S3 (https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html).\n",
    "3. Install AWS CLI (https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html).\n",
    "3. Configure the IAM user in AWS CLI.(https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html).\n",
    "5. Have One AMI with all required software(spark, python, ssh) installed in it (Refer to AMI creation step 3.1).\n",
    "6. Make sure SSH is available or install the same.\n",
    "\n",
    "\n",
    "## 3. Usage:\n",
    "\n",
    "Okay, boring part is over. If you are still with me that means you really need a cluster of you rown. No worries, no worries at all. Now we can start using the utility. From here onwards we will go step-by-step.\n",
    "### 3.1. AMI Creation:\n",
    "\n",
    "In this step we will create one AMI(Amazon Machine Image) that will be used as base image for each node. If you wish to install any other softwares in each node or want to do any configurations, I suggest you do those in this image. Here we will do follwoing steps:\n",
    "1. Launch an EC2 instance using Amazon Linux 2 AMI(https://docs.aws.amazon.com/quickstarts/latest/vmlaunch/step-1-launch-instance.html). Make sure the instance is launched with an public DNS.\n",
    "2. Login into the EC2 instance using the public DNS (https://docs.aws.amazon.com/quickstarts/latest/vmlaunch/step-2-connect-to-instance.html).\n",
    "2. Run following commands:\n",
    "    - #### Update all existing libraries:\n",
    "        - _sudo yum update -y_\n",
    "\n",
    "    - #### Install Java 8:\n",
    "        - _sudo yum install java-1.8.0-openjdk-devel_\n",
    "        - _java -version_ (In case of unsatisfactory results, try to install java using some other method).\n",
    "\n",
    "    - #### Download and Install Scala:\n",
    "        - _wget http://downloads.lightbend.com/scala/2.11.8/scala-2.11.8.rpm_\n",
    "        - _sudo yum install scala-2.11.8.rpm_\n",
    "        - _scala -version_ (In case of unsatisfactory results, try to install java using some other method).\n",
    "\n",
    "    - #### Download and extract Spark:\n",
    "        - _wget http://archive.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz_\n",
    "        - _sudo tar -zxvf spark-2.4.5-bin-hadoop2.7.tgz_\n",
    "\n",
    "    __Please note__, to use PySpark, Install Python 3.7, pip3, py4j and findspark after these steps.\n",
    "3. Create an AMI from this EC2(https://docs.aws.amazon.com/toolkit-for-visual-studio/latest/user-guide/tkv-create-ami-from-instance.html). Please use proper ProjectName tag (provide the same ProjectName in configuration file in next section), or else it will not be visible to utility.\n",
    "4. Terminate the EC2 instance.\n",
    "\n",
    "__*N.B.* This is a one time activity. Once an AMI is created. It can be re-used for each run.__\n",
    "### 3.2. Cluster Configuration:\n",
    "\n",
    "To run this Utility, one configuration file must be provided. Some configurations are mandatory and some are optional. Mandatory configurations are provided in __Bold__. The utility expects the configuration file ('cluster_config.json') to be present in the code base directory itself.\n",
    "#### Following are the properties to be configured:\n",
    "* Region: AWS Region. Default value is __us-east-1__.\n",
    "* AZList: AWS Availability Zones. Default values is __[us-east-1a, us-east-1b]__. Please provide valid Availability Zone values if configuring explicitly.\n",
    "* __CidrBlock__: CIDR Block for the VPC to be launched. This is a __mandatory__ field. Please note, irrespective of the provided value, masking will be '__/22__'\n",
    "* __KeyPairPath__: Key pair file (.pem) path. This is also __mandatory__ field.\n",
    "* __KeyPairName__: Key pair file (.pem) name. Please __DO NOT__ provide the extension. __Only the name__ is required. This is also __mandatory__ field.\n",
    "* ProjectTag: Project name. Default value is '__SparkCluster__'.\n",
    "* __IPWhitelist__: List of IPs to be whitelisted. Any machine that is supposed to behave as a Edge node, needs to be whitelisted. If you want to whitelist entire internet(please don't do that, major security concern) provide __[0.0.0.0/0]__.\n",
    "* UserName: Utility user name, to distinguise separate verisons of execution. Default value is '__root__'. Please note __only one cluster__ is allowed per user.\n",
    "* InstanceType: EC2 instance type for the nodes. Default value is '__t2.micro__'.\n",
    "* __WorkspaceDirectory__: Local directory that will be used as workspace by the uitlity durinf run time. This is a __mandatory__ field.\n",
    "* SlaveCount: Number of Slave nodes. Default value is __3__.\n",
    "\n",
    "### 3.3. Create Network:\n",
    "From here onwards we will execute provided Notebooks to Complete a step. Execute following Notebook:\n",
    "* __*cloudformation_stack_creation.ipynb*__\n",
    "\n",
    "### 3.4. Launch the Nodes (Master and Slave):\n",
    "Execute following Notebook in provided sequence:\n",
    "1. __*master_node_creation.ipynb*__\n",
    "2. __*slave_nodes_creation.ipynb*__\n",
    "\n",
    "### 3.5. Configure the Nodes:\n",
    "Execute following Notebook:\n",
    "* __*spark_cluster_configure.ipynb*__\n",
    "\n",
    "### 3.6. Start the Cluster:\n",
    "Execute following Notebook:\n",
    "* __*spark_cluster_start.ipynb*__\n",
    "\n",
    "### 3.7. Stop the Cluster:\n",
    "Execute following Notebook:\n",
    "* __*spark_cluster_stop.ipynb*__\n",
    "\n",
    "### 3.8. Terminate the Nodes:\n",
    "Execute following Notebooks in provided sequence:\n",
    "1. __*slave_node_termination.ipynb*__\n",
    "2. __*master_node_termination.ipynb*__\n",
    "\n",
    "### 3.9. Clean up other AWS services:\n",
    "Execute following Notebook:\n",
    "* __*cloudformation_stack_deletion.ipynb*__\n",
    "\n",
    "## 4. Next Version Trailers:\n",
    "We are planning to include following features(feels really important now that we talk about these) in next version. Stay tuned.\n",
    "* Add worker node to running cluster.\n",
    "* Whitelist IP in a running cluster, without stoping the cluster.\n",
    "* Currently the AMI is predefined manually(although one time activity), In future releases, this step will be automated.\n",
    "* One python package version of the utility will be provided to integrate it with other applications.\n",
    "\n",
    "## Conclusion:\n",
    "Let me end the article with thanking you for going through the article. It will be highly appreciated if anyone have any suggestions, ideas to implement. In case of any queries, suggestion, I am available on _+91-9593090126_ and saumalya75@gmail.com. The entire project is available on my BitBucket repository - https://bitbucket.org/saumalya75/sparkclusteronaws/src/master/__. The repository can be cloned from __git@bitbucket.org:saumalya75/sparkclusteronaws.git__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
