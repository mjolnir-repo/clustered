{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Cluster Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Required Libraries ::\n",
    "* __boto3__: Required to connect as operate AWS task\n",
    "* __botocore__: Required to handle the exceptions related to boto3 tasks\n",
    "* __paramiko__: Reuired to run commands inside EC2 instances\n",
    "* __json__: To convert python native dictionaries to string, to write in files\n",
    "* __datetime__, __pprint__, __sys__, __time__: General purpose use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, botocore, paramiko\n",
    "from datetime import datetime\n",
    "import pprint, sys, time, json\n",
    "from botocore.exceptions import ClientError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating boto3 session, clients and resources ::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    session = boto3.session.Session(region_name='us-east-1')\n",
    "    ec2_client = session.client('ec2')\n",
    "    ec2_resource = session.resource('ec2')\n",
    "except ClientError as e:\n",
    "    print(\"Unexpected error while creating boto3 session, client and resources: \" + str(e))\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaring the hardcoded informations ::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "user = 'root'\n",
    "# config_dir = '/Volumes/WorkSpace/POC/SparkClusterEC2/ConfigDir'\n",
    "# config_file_name = config_dir + '/' + user + '_node_details.dat'\n",
    "spark_home = '/home/ec2-user/spark-2.4.5-bin-hadoop2.7'\n",
    "cluster_instance_type = 't2.micro'\n",
    "cluster_key_pair_path = '/Volumes/WorkSpace/AWS/Access_Keys'\n",
    "cluster_key_pair_name = 'SparkCluster'\n",
    "cluster_subnet_id = 'subnet-070cddc01a126f07f'\n",
    "cluster_security_group_list = ['sg-05ee7f205f173862c']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for running Master Node for current user ::\n",
    "* To start any cluster, first master node needs to be detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master node: Instance('i-0a27306c67986fd4b').\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    master_instance_details = ec2_resource.instances.filter(\n",
    "        Filters=[\n",
    "            {\n",
    "                'Name': 'instance-state-name',\n",
    "                'Values': ['running']\n",
    "            },\n",
    "            {\n",
    "                'Name': 'tag:Project',\n",
    "                'Values': ['SparkCluster']\n",
    "            },\n",
    "            {\n",
    "                'Name': 'tag:User',\n",
    "                'Values': [user]\n",
    "            },\n",
    "            {\n",
    "                'Name': 'tag:NodeType',\n",
    "                'Values': ['Master']\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    if list(master_instance_details):\n",
    "        master_node_id = list(master_instance_details)[0].id\n",
    "        print(\"Master node: Instance('\" + master_node_id + \"').\")\n",
    "    else:\n",
    "        print(\"No running master node for User('\" + user + \"'). Quitting process.\")\n",
    "        exit()\n",
    "except ClientError as e:\n",
    "    print(\"Unexpected error while looking for already running Master node EC2 instance for user-'\" + user + \"': \" + str(e))\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching required information of the Master Node ::\n",
    "* Need to iterate and probe a few times to check whether the node is up before we can extract the informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'InstanceId': 'i-0a27306c67986fd4b',\n",
      " 'NodeName': 'master',\n",
      " 'PrivateIpAddress': '172.75.0.12',\n",
      " 'PublicDnsName': 'ec2-3-95-251-159.compute-1.amazonaws.com',\n",
      " 'PublicIpAddress': '3.95.251.159'}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    master_node_temp = ec2_client.describe_instances(InstanceIds=[master_node_id])['Reservations'][0]['Instances'][0]\n",
    "    master_node = {\n",
    "            'InstanceId': master_node_temp['InstanceId'],\n",
    "            'PublicDnsName': master_node_temp['PublicDnsName'],\n",
    "            'PublicIpAddress': master_node_temp['PublicIpAddress'],\n",
    "            'PrivateIpAddress': master_node_temp['PrivateIpAddress'],\n",
    "            'NodeName': 'master'\n",
    "        }\n",
    "    pprint.pprint(master_node)\n",
    "except Exception as e:\n",
    "    print(\"Unexpected error while extracting Spark Cluster Master node details: \" + str(e))\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for running Slave Nodes for current user ::\n",
    "* To start any cluster, first master node needs to be detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slave node: Instance('i-0d1b1bbaf19af5f60').\n",
      "Slave node: Instance('i-0d199f2f7e4a24207').\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    slave_node_id_list = []\n",
    "    slave_instance_details = ec2_resource.instances.filter(\n",
    "        Filters=[\n",
    "            {\n",
    "                'Name': 'instance-state-name',\n",
    "                'Values': ['running']\n",
    "            },\n",
    "            {\n",
    "                'Name': 'tag:Project',\n",
    "                'Values': ['SparkCluster']\n",
    "            },\n",
    "            {\n",
    "                'Name': 'tag:User',\n",
    "                'Values': [user]\n",
    "            },\n",
    "            {\n",
    "                'Name': 'tag:NodeType',\n",
    "                'Values': ['Slave']\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    if list(slave_instance_details):\n",
    "        for slave_instance in slave_instance_details:\n",
    "            slave_node_id_list.append(slave_instance.id)\n",
    "            print(\"Slave node: Instance('\" + slave_instance.id + \"').\")\n",
    "    else:\n",
    "        print(\"No running slave node for User('\" + user + \"'). Quitting process.\")\n",
    "        exit()\n",
    "except ClientError as e:\n",
    "    print(\"Unexpected error while looking for already running Slave node EC2 instance for user-'\" + user + \"': \" + str(e))\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching required information of the Slave Nodes ::\n",
    "* Need to iterate and probe a few times to check whether the node is up before we can extract the informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'InstanceId': 'i-0d1b1bbaf19af5f60',\n",
      "  'NodeName': 'SlaveNode1',\n",
      "  'PrivateIpAddress': '172.75.0.22',\n",
      "  'PublicDnsName': 'ec2-35-175-225-147.compute-1.amazonaws.com',\n",
      "  'PublicIpAddress': '35.175.225.147'},\n",
      " {'InstanceId': 'i-0d199f2f7e4a24207',\n",
      "  'NodeName': 'SlaveNode2',\n",
      "  'PrivateIpAddress': '172.75.0.102',\n",
      "  'PublicDnsName': 'ec2-18-234-92-252.compute-1.amazonaws.com',\n",
      "  'PublicIpAddress': '18.234.92.252'}]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    slave_node_list = []\n",
    "    slave_node_list_temp = ec2_client.describe_instances(InstanceIds=slave_node_id_list)['Reservations'][0]['Instances']\n",
    "    for i in range(len(slave_node_list_temp)):\n",
    "        slave_node_list.append({\n",
    "            'InstanceId': slave_node_list_temp[i]['InstanceId'],\n",
    "            'PublicDnsName': slave_node_list_temp[i]['PublicDnsName'],\n",
    "            'PublicIpAddress': slave_node_list_temp[i]['PublicIpAddress'],\n",
    "            'PrivateIpAddress': slave_node_list_temp[i]['PrivateIpAddress'],\n",
    "            'NodeName': 'SlaveNode' + str(i + 1)\n",
    "        })\n",
    "    pprint.pprint(slave_node_list)\n",
    "except Exception as e:\n",
    "    print(\"Unexpected error while extracting Spark Cluster Master node details: \" + str(e))\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring all created nodes and create Spark Cluster ::\n",
    "* Login using pre-defined .pem file\n",
    "* Connect using ssh protocol\n",
    "* Configure /ect/hosts file of each node(master and slave)\n",
    "* Configure passwordless ssh between master and slave nodes\n",
    "* Configure all spark related properties in spark-env.sh (only Master Ip is configured here).\n",
    "* Configure all slave nodes in SPARK_HOME/conf/slaves file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to configure Spark Node(ec2-3-95-251-159.compute-1.amazonaws.com :: master).\n",
      "Spark Node(ec2-3-95-251-159.compute-1.amazonaws.com :: master) is ready to be used.\n",
      "Starting to configure Spark Node(ec2-35-175-225-147.compute-1.amazonaws.com :: SlaveNode1).\n",
      "Spark Node(ec2-35-175-225-147.compute-1.amazonaws.com :: SlaveNode1) is ready to be used.\n",
      "Starting to configure Spark Node(ec2-18-234-92-252.compute-1.amazonaws.com :: SlaveNode2).\n",
      "Spark Node(ec2-18-234-92-252.compute-1.amazonaws.com :: SlaveNode2) is ready to be used.\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Spark cluster configuration is done. Run 'spark_cluster_start.ipynb' to start the cluster.\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for node in [master_node] + slave_node_list:\n",
    "        print(\"Starting to configure Spark Node(\"+ node['PublicDnsName'] +\" :: \" + node['NodeName'] + \").\")\n",
    "        ssh = paramiko.SSHClient()\n",
    "        ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "        privkey = paramiko.RSAKey.from_private_key_file(cluster_key_pair_path + '/' + cluster_key_pair_name + '.pem')\n",
    "        connect_limit = 5\n",
    "        for _ in range(1, 5):\n",
    "            try:\n",
    "                ssh.connect(node['PublicDnsName'], username='ec2-user', pkey=privkey)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(\"Unexpected error while trying to connect Spark Cluster Master Node: '\" + node['PublicDnsName'] + \"'. Retrying after 5 secs...\")\n",
    "                time.sleep(5)\n",
    "        else:\n",
    "            print(\"Maximum connection try limit exceeded, still could not connect to master node. Check AWS Management console for further details.\")\n",
    "            print(node)\n",
    "            exit()\n",
    "\n",
    "        _, stdout, _ = ssh.exec_command(\"sudo cp -p /etc/hosts /etc/hosts_bkp\")\n",
    "        if stdout.channel.recv_exit_status() != 0:\n",
    "            print(\"Unexpected error occured while creating back up of '/etc/hosts' file.\")\n",
    "            raise Exception\n",
    "        hosts_file_master_cmd = \"echo '\" + master_node['PrivateIpAddress'] + \" master' >> /etc/hosts\"\n",
    "        _, stdout, _ = ssh.exec_command(hosts_file_master_cmd)\n",
    "        if stdout.channel.recv_exit_status() != 0:\n",
    "            print(\"Unexpected error occured while adding master node private ip to '/etc/hosts' file.\")\n",
    "            raise Exception\n",
    "\n",
    "        if node['NodeName'] == 'master':\n",
    "            _, stdout, _ = ssh.exec_command(\"cp -p \" + spark_home + \"/conf/slaves \" + spark_home + \"/conf/slaves_bkp\")\n",
    "            if stdout.channel.recv_exit_status()  != 0:\n",
    "                print(\"Unexpected error occured while taking back up of '\" + spark_home + \"/conf/slaves' file.\")\n",
    "                raise Exception\n",
    "\n",
    "        for slave_node in slave_node_list:\n",
    "            hosts_file_slave_cmd = \"echo '\" + slave_node['PrivateIpAddress'] + \" \" + slave_node['NodeName'] + \"' >> /etc/hosts\"\n",
    "            _, stdout, _ = ssh.exec_command(hosts_file_slave_cmd)\n",
    "            if stdout.channel.recv_exit_status()  != 0:\n",
    "                print(\"Unexpected error occured while adding slave node private ip('\" + slave_node['PrivateIpAddress'] + \"') to '/etc/hosts' file.\")\n",
    "                raise Exception\n",
    "            if node['NodeName'] == 'master':\n",
    "                hosts_file_slave_cmd = \"echo '\" + slave_node['NodeName'] + \"' >> \" + spark_home + \"/conf/slaves\"\n",
    "                _, stdout, _ = ssh.exec_command(hosts_file_slave_cmd)\n",
    "                if stdout.channel.recv_exit_status()  != 0:\n",
    "                    print(\"Unexpected error occured while adding slave node name(\" + slave_node['NodeName'] + \") to '/conf/slaves' file.\")\n",
    "                    raise Exception\n",
    "\n",
    "        spark_env_file_cmd = \"sed 's/<SPARK_CLUSTER_MASTER_PRIVATE_IP>/\" + master_node['PrivateIpAddress'] + \"/g' \" + spark_home + \"/conf/spark-env.sh > \" + spark_home + \"/conf/spark-env-new.sh\"\n",
    "        _, stdout, _ = ssh.exec_command(spark_env_file_cmd)\n",
    "        if stdout.channel.recv_exit_status()  != 0:\n",
    "            print(\"Unexpected error occured while putting Master node private ip in '\" + spark_home + \"/conf/spark-env.sh' file.\")\n",
    "            raise Exception\n",
    "        _, stdout, _ = ssh.exec_command(\"chmod 755 \" + spark_home + \"/conf/spark-env-new.sh\")\n",
    "        if stdout.channel.recv_exit_status()  != 0:\n",
    "            print(\"Unexpected error occured while putting Master node private ip in '\" + spark_home + \"/conf/spark-env.sh' file.\")\n",
    "            raise Exception\n",
    "        _, stdout, _ = ssh.exec_command(\"mv \" + spark_home + \"/conf/spark-env.sh  \" + spark_home + \"/conf/spark-env-bkp.sh\")\n",
    "        if stdout.channel.recv_exit_status()  != 0:\n",
    "            print(\"Unexpected error occured while putting Master node private ip in '\" + spark_home + \"/conf/spark-env.sh' file.\")\n",
    "            raise Exception\n",
    "        _, stdout, _ = ssh.exec_command(\"mv \" + spark_home + \"/conf/spark-env-new.sh \" + spark_home + \"/conf/spark-env.sh\")\n",
    "        if stdout.channel.recv_exit_status()  != 0:\n",
    "            print(\"Unexpected error occured while putting Master node private ip in '\" + spark_home + \"/conf/spark-env.sh' file.\")\n",
    "            raise Exception\n",
    "\n",
    "        auth_key_file_cmd = \"sed 's/<SPARK_CLUSTER_MASTER_PRIVATE_IP>/\" + master_node['PrivateIpAddress'] + \"/g' /home/ec2-user/.ssh/authorized_keys > /home/ec2-user/.ssh/authorized_keys_new\"\n",
    "        _, stdout, _ = ssh.exec_command(auth_key_file_cmd)\n",
    "        if stdout.channel.recv_exit_status()  != 0:\n",
    "            print(\"Unexpected error occured while putting Master node private ip in '/home/ec2-user/.ssh/authorized_keys' file.\")\n",
    "            raise Exception\n",
    "        _, stdout, _ = ssh.exec_command(\"mv /home/ec2-user/.ssh/authorized_keys  /home/ec2-user/.ssh/authorized_keys_bkp\")\n",
    "        if stdout.channel.recv_exit_status()  != 0:\n",
    "            print(\"Unexpected error occured while putting Master node private ip in '/home/ec2-user/.ssh/authorized_keys' file.\")\n",
    "            raise Exception\n",
    "        _, stdout, _ = ssh.exec_command(\"mv /home/ec2-user/.ssh/authorized_keys_new /home/ec2-user/.ssh/authorized_keys\")\n",
    "        if stdout.channel.recv_exit_status()  != 0:\n",
    "            print(\"Unexpected error occured while putting Master node private ip in '/home/ec2-user/.ssh/authorized_keys' file.\")\n",
    "            raise Exception\n",
    "        _, stdout, _ = ssh.exec_command(\"chmod 600 /home/ec2-user/.ssh/authorized_keys\")\n",
    "        if stdout.channel.recv_exit_status()  != 0:\n",
    "            print(\"Unexpected error occured while putting Master node private ip in '/home/ec2-user/.ssh/authorized_keys' file.\")\n",
    "            raise Exception\n",
    "\n",
    "        ssh.close()\n",
    "        print(\"Spark Node(\"+ node['PublicDnsName'] +\" :: \" + node['NodeName'] + \") is ready to be used.\")\n",
    "    \n",
    "    print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "    print(\"Spark cluster configuration is done. Run 'spark_cluster_start.ipynb' to start the cluster.\")\n",
    "    print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "except Exception as e:\n",
    "    print(\"Unexpected error while changing spark env in Spark Cluster Master Node: \" + str(e))\n",
    "    if ssh:\n",
    "        ssh.close()\n",
    "    exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
