{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Cluster Start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Required Libraries ::\n",
    "* __boto3__: Required to connect as operate AWS task\n",
    "* __botocore__: Required to handle the exceptions related to boto3 tasks\n",
    "* __paramiko__: Reuired to run commands inside EC2 instances\n",
    "* __json__: To convert python native dictionaries to string, to write in files\n",
    "* __datetime__, __pprint__, __sys__, __time__: General purpose use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, botocore, paramiko\n",
    "from datetime import datetime\n",
    "import pprint, sys, time, json\n",
    "from botocore.exceptions import ClientError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating boto3 session, clients and resources ::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    session = boto3.session.Session(region_name='us-east-1')\n",
    "    ec2_client = session.client('ec2')\n",
    "    ec2_resource = session.resource('ec2')\n",
    "except ClientError as e:\n",
    "    print(\"Unexpected error while creating boto3 session, client and resources: \" + str(e))\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaring the hardcoded informations ::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "user = 'root'\n",
    "# config_dir = '/Volumes/WorkSpace/POC/SparkClusterEC2/ConfigDir'\n",
    "# config_file_name = config_dir + '/' + user + '_node_details.dat'\n",
    "spark_home = '/home/ec2-user/spark-2.4.5-bin-hadoop2.7'\n",
    "cluster_instance_type = 't2.micro'\n",
    "cluster_key_pair_path = '/Volumes/WorkSpace/AWS/Access_Keys'\n",
    "cluster_key_pair_name = 'SparkCluster'\n",
    "cluster_subnet_id = 'subnet-070cddc01a126f07f'\n",
    "cluster_security_group_list = ['sg-05ee7f205f173862c']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for running Master Node for current user ::\n",
    "* To start any cluster, first master node needs to be detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master node: Instance('i-0a27306c67986fd4b').\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    master_instance_details = ec2_resource.instances.filter(\n",
    "        Filters=[\n",
    "            {\n",
    "                'Name': 'instance-state-name',\n",
    "                'Values': ['running']\n",
    "            },\n",
    "            {\n",
    "                'Name': 'tag:Project',\n",
    "                'Values': ['SparkCluster']\n",
    "            },\n",
    "            {\n",
    "                'Name': 'tag:User',\n",
    "                'Values': [user]\n",
    "            },\n",
    "            {\n",
    "                'Name': 'tag:NodeType',\n",
    "                'Values': ['Master']\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    if list(master_instance_details):\n",
    "        master_node_id = list(master_instance_details)[0].id\n",
    "        print(\"Master node: Instance('\" + master_node_id + \"').\")\n",
    "    else:\n",
    "        print(\"No running master node for User('\" + user + \"'). Quitting process.\")\n",
    "        exit()\n",
    "except ClientError as e:\n",
    "    print(\"Unexpected error while looking for already running Master node EC2 instance for user-'\" + user + \"': \" + str(e))\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching required information of the Master Node ::\n",
    "* Need to iterate and probe a few times to check whether the node is up before we can extract the informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'InstanceId': 'i-0a27306c67986fd4b',\n",
      " 'NodeName': 'master',\n",
      " 'PrivateIpAddress': '172.75.0.12',\n",
      " 'PublicDnsName': 'ec2-3-95-251-159.compute-1.amazonaws.com',\n",
      " 'PublicIpAddress': '3.95.251.159'}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    master_node_temp = ec2_client.describe_instances(InstanceIds=[master_node_id])['Reservations'][0]['Instances'][0]\n",
    "    master_node = {\n",
    "            'InstanceId': master_node_temp['InstanceId'],\n",
    "            'PublicDnsName': master_node_temp['PublicDnsName'],\n",
    "            'PublicIpAddress': master_node_temp['PublicIpAddress'],\n",
    "            'PrivateIpAddress': master_node_temp['PrivateIpAddress'],\n",
    "            'NodeName': 'master'\n",
    "        }\n",
    "    pprint.pprint(master_node)\n",
    "except Exception as e:\n",
    "    print(\"Unexpected error while extracting Spark Cluster Master node details: \" + str(e))\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop the cluster ::\n",
    "* Login using pre-defined .pem file\n",
    "* Connect using ssh protocol\n",
    "* un stop-all.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/spark-2.4.5-bin-hadoop2.7/sbin/stop-all.sh\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Spark cluster is stoped. Please clean up the nodes by running 'slave_nodes_termination.ipynb' and 'master_nodes_termination.ipynb' notebooks in provided sequence.\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    ssh = paramiko.SSHClient()\n",
    "    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "    privkey = paramiko.RSAKey.from_private_key_file(cluster_key_pair_path + '/' + cluster_key_pair_name + '.pem')\n",
    "    connect_limit = 5\n",
    "    for _ in range(1, 5):\n",
    "        try:\n",
    "            ssh.connect(master_node['PublicDnsName'], username='ec2-user', pkey=privkey)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(\"Unexpected error while trying to connect Spark Cluster Master Node: '\" + master_node['PublicDnsName'] + \"'. Retrying after 5 secs...\")\n",
    "            time.sleep(5)\n",
    "    else:\n",
    "        print(\"Maximum connection try limit exceeded, still could not connect to master node. Check AWS Management console for further details.\")\n",
    "        print(master_node)\n",
    "        exit()\n",
    "\n",
    "    cluster_down_cmd = spark_home + \"/sbin/stop-all.sh\"\n",
    "    print(cluster_down_cmd)\n",
    "    _, stdout, _ = ssh.exec_command(cluster_down_cmd)\n",
    "    exit_status = stdout.channel.recv_exit_status()   \n",
    "    if exit_status != 0:\n",
    "        print(\"Spark cluster stop command failed on master node Node(\" + master_node['PublicDnsName'] + \"). Please log in manualy to the node and do the needfull.\")\n",
    "        ssh.close()\n",
    "    else:\n",
    "        ssh.close()\n",
    "        print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "        print(\"Spark cluster is stoped. Please clean up the nodes by running 'slave_nodes_termination.ipynb' and 'master_nodes_termination.ipynb' notebooks in provided sequence.\")\n",
    "        print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "except Exception as e:\n",
    "    print(\"Unexpected error while changing spark env in Spark Cluster Master Node: \" + str(e))\n",
    "    ssh.close()\n",
    "    exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
